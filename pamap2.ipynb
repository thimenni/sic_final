{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76aca012-063f-4b1d-b2ee-d2930ba5fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler as sparkMinMaxScaler\n",
    "from pyspark.ml import Pipeline as SparkPipeline\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import sys\n",
    "import warnings\n",
    "from functools import reduce\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8364d019-e0fd-47b1-9de8-acda4d85dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_spark_session(app_name=\"PAMAP2 Analysis\",\n",
    "                         master_url=\"spark://spark-master:7077\",\n",
    "                         executor_memory=\"2g\",\n",
    "                         driver_memory=\"2g\",\n",
    "                         hdfs_url=\"hdfs://namenode:8020\"):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(master_url) \\\n",
    "        .config(\"spark.executor.memory\", executor_memory) \\\n",
    "        .config(\"spark.driver.memory\", driver_memory) \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", hdfs_url) \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b11419-a4f5-45a2-bcd1-dbf5fcf05582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_schema():\n",
    "    \"\"\"\n",
    "    Định nghĩa schema cho dữ liệu PAMAP2\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", DoubleType(), True),\n",
    "        StructField(\"activityID\", IntegerType(), True),\n",
    "        StructField(\"heartrate\", DoubleType(), True),\n",
    "        StructField(\"handTemperature\", DoubleType(), True),\n",
    "        StructField(\"handAcc16_1\", DoubleType(), True),\n",
    "        StructField(\"handAcc16_2\", DoubleType(), True),\n",
    "        StructField(\"handAcc16_3\", DoubleType(), True),\n",
    "        StructField(\"handAcc6_1\", DoubleType(), True),\n",
    "        StructField(\"handAcc6_2\", DoubleType(), True),\n",
    "        StructField(\"handAcc6_3\", DoubleType(), True),\n",
    "        StructField(\"handGyro1\", DoubleType(), True),\n",
    "        StructField(\"handGyro2\", DoubleType(), True),\n",
    "        StructField(\"handGyro3\", DoubleType(), True),\n",
    "        StructField(\"handMagne1\", DoubleType(), True),\n",
    "        StructField(\"handMagne2\", DoubleType(), True),\n",
    "        StructField(\"handMagne3\", DoubleType(), True),\n",
    "        StructField(\"handOrientation1\", DoubleType(), True),\n",
    "        StructField(\"handOrientation2\", DoubleType(), True),\n",
    "        StructField(\"handOrientation3\", DoubleType(), True),\n",
    "        StructField(\"handOrientation4\", DoubleType(), True),\n",
    "        StructField(\"chestTemperature\", DoubleType(), True),\n",
    "        StructField(\"chestAcc16_1\", DoubleType(), True),\n",
    "        StructField(\"chestAcc16_2\", DoubleType(), True),\n",
    "        StructField(\"chestAcc16_3\", DoubleType(), True),\n",
    "        StructField(\"chestAcc6_1\", DoubleType(), True),\n",
    "        StructField(\"chestAcc6_2\", DoubleType(), True),\n",
    "        StructField(\"chestAcc6_3\", DoubleType(), True),\n",
    "        StructField(\"chestGyro1\", DoubleType(), True),\n",
    "        StructField(\"chestGyro2\", DoubleType(), True),\n",
    "        StructField(\"chestGyro3\", DoubleType(), True),\n",
    "        StructField(\"chestMagne1\", DoubleType(), True),\n",
    "        StructField(\"chestMagne2\", DoubleType(), True),\n",
    "        StructField(\"chestMagne3\", DoubleType(), True),\n",
    "        StructField(\"chestOrientation1\", DoubleType(), True),\n",
    "        StructField(\"chestOrientation2\", DoubleType(), True),\n",
    "        StructField(\"chestOrientation3\", DoubleType(), True),\n",
    "        StructField(\"chestOrientation4\", DoubleType(), True),\n",
    "        StructField(\"ankleTemperature\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc16_1\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc16_2\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc16_3\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc6_1\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc6_2\", DoubleType(), True),\n",
    "        StructField(\"ankleAcc6_3\", DoubleType(), True),\n",
    "        StructField(\"ankleGyro1\", DoubleType(), True),\n",
    "        StructField(\"ankleGyro2\", DoubleType(), True),\n",
    "        StructField(\"ankleGyro3\", DoubleType(), True),\n",
    "        StructField(\"ankleMagne1\", DoubleType(), True),\n",
    "        StructField(\"ankleMagne2\", DoubleType(), True),\n",
    "        StructField(\"ankleMagne3\", DoubleType(), True),\n",
    "        StructField(\"ankleOrientation1\", DoubleType(), True),\n",
    "        StructField(\"ankleOrientation2\", DoubleType(), True),\n",
    "        StructField(\"ankleOrientation3\", DoubleType(), True),\n",
    "        StructField(\"ankleOrientation4\", DoubleType(), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "def load_data(spark, hdfs_path):\n",
    "    \"\"\"\n",
    "    Load dữ liệu từ HDFS với schema định nghĩa\n",
    "    \"\"\"\n",
    "    schema = define_schema()\n",
    "\n",
    "    print(\"Đang load dữ liệu từ HDFS...\")\n",
    "    df = spark.read \\\n",
    "        .option(\"delimiter\", \" \") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(hdfs_path) \\\n",
    "        .cache()\n",
    "\n",
    "    count = df.count()\n",
    "    print(f\"Đã load {count} dòng dữ liệu từ {hdfs_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e7397e-3789-4b3f-8f3d-d04b00f7567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(df):\n",
    "    \"\"\"\n",
    "    Làm sạch dữ liệu cơ bản\n",
    "    - Loại bỏ activityID = 0 và null\n",
    "    \"\"\"\n",
    "    print(\"Đang thực hiện basic cleaning...\")\n",
    "\n",
    "    df_cleaned = df.filter((df.activityID.isNotNull()) & (df.activityID != 0)).cache()\n",
    "\n",
    "    print(f\"Số dòng sau khi lọc activityID != 0 và != null: {df_cleaned.count()}\")\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2184adb9-a469-49f1-a525-344ef748060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_features(df):\n",
    "    \"\"\"\n",
    "    Loại bỏ các features không có ý nghĩa cho phân loại hoạt động thể chất\n",
    "    và cache lại DataFrame để tối ưu hiệu năng khi xử lý tiếp.\n",
    "    \"\"\"\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    print(\"Đang loại bỏ các features không liên quan...\")\n",
    "\n",
    "    # Giữ lại các features quan trọng cho phân loại hoạt động\n",
    "    relevant_features = [\n",
    "        'activityID', 'heartrate',\n",
    "        'handAcc16_1', 'handAcc16_2', 'handAcc16_3',\n",
    "        'handAcc6_1', 'handAcc6_2', 'handAcc6_3',\n",
    "        'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3',\n",
    "        'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3',\n",
    "        'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3',\n",
    "        'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3',\n",
    "        'handGyro1', 'handGyro2', 'handGyro3',\n",
    "        'chestGyro1', 'chestGyro2', 'chestGyro3',\n",
    "        'ankleGyro1', 'ankleGyro2', 'ankleGyro3',\n",
    "        'handMagne1', 'handMagne2', 'handMagne3',\n",
    "        'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
    "        'ankleMagne1', 'ankleMagne2', 'ankleMagne3'\n",
    "    ]\n",
    "\n",
    "    df_selected = df.select(*relevant_features).cache()\n",
    "\n",
    "    print(f\"Giữ lại {len(relevant_features)} / {len(df.columns)} cột\")\n",
    "    print(f\"Số dòng dữ liệu sau khi lọc cột: {df_selected.count()}\")\n",
    "\n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f4c0f2-b87b-476d-96a3-7cd5a019ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import last, col, coalesce\n",
    "import sys\n",
    "\n",
    "def fill_missing_values_ffill_bfill(df, time_col='timestamp', activity_col='activityID'):\n",
    "    print(\"Đang điền missing values bằng forward + backward fill (tối ưu)...\")\n",
    "\n",
    "    # Định nghĩa window\n",
    "    window_ffill = Window.partitionBy(activity_col).orderBy(time_col).rowsBetween(-sys.maxsize, 0)\n",
    "    window_bfill = Window.partitionBy(activity_col).orderBy(time_col).rowsBetween(0, sys.maxsize)\n",
    "\n",
    "    # Các cột cần điền\n",
    "    cols_to_fill = [c for c in df.columns if c not in [time_col, activity_col]]\n",
    "\n",
    "    for c in cols_to_fill:\n",
    "        ffill_col = last(c, ignorenulls=True).over(window_ffill)\n",
    "        bfill_col = last(c, ignorenulls=True).over(window_bfill)\n",
    "        df = df.withColumn(c, coalesce(ffill_col, bfill_col))\n",
    "\n",
    "    print(f\"Đã hoàn tất điền missing cho {len(cols_to_fill)} cột.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a70a352-fb9c-4ce5-9f14-057d7980c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def remove_outliers_iqr(df: DataFrame, outlier_cols: list, quantile_approx=0.05) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loại bỏ outliers bằng IQR method trên từng cột được chỉ định.\n",
    "    Áp dụng IQR theo từng nhóm `activityID`.\n",
    "    \"\"\"\n",
    "    print(\"Đang loại bỏ outliers bằng IQR...\")\n",
    "\n",
    "    for col in outlier_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Bỏ qua cột không tồn tại: {col}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Đang xử lý cột: {col}\")\n",
    "\n",
    "        # Tính Q1 và Q3 cho mỗi activityID\n",
    "        quantiles = df.groupBy(\"activityID\").agg(\n",
    "            fn.expr(f\"percentile_approx({col}, 0.25, {quantile_approx})\").alias(\"q1\"),\n",
    "            fn.expr(f\"percentile_approx({col}, 0.75, {quantile_approx})\").alias(\"q3\")\n",
    "        )\n",
    "\n",
    "        bounds = quantiles.withColumn(\"iqr\", fn.col(\"q3\") - fn.col(\"q1\")) \\\n",
    "                          .withColumn(\"lower_bound\", fn.col(\"q1\") - 1.5 * fn.col(\"iqr\")) \\\n",
    "                          .withColumn(\"upper_bound\", fn.col(\"q3\") + 1.5 * fn.col(\"iqr\")) \\\n",
    "                          .select(\"activityID\", \"lower_bound\", \"upper_bound\")\n",
    "\n",
    "        df = df.join(bounds, on=\"activityID\", how=\"left\")\n",
    "        before = df.count()\n",
    "\n",
    "        df = df.filter((fn.col(col) >= fn.col(\"lower_bound\")) & (fn.col(col) <= fn.col(\"upper_bound\")))\n",
    "        after = df.count()\n",
    "\n",
    "        print(f\"Cột {col}: Đã loại {before - after} dòng outlier ({before} → {after})\")\n",
    "\n",
    "        df = df.drop(\"lower_bound\", \"upper_bound\")\n",
    "\n",
    "    print(\"Hoàn tất xử lý outliers.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f964a5c-6026-4f15-ba51-316eaaf42d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Tối ưu hóa tạo các feature magnitude từ accelerometer và gyroscope\n",
    "    bằng cách gom các phép tính vào một bước duy nhất.\n",
    "    \"\"\"\n",
    "    print(\"Đang tạo features mới (tối ưu)...\")\n",
    "\n",
    "    df = df.withColumns({\n",
    "        'hand_acc_magnitude': fn.sqrt(fn.col('handAcc16_1')**2 + fn.col('handAcc16_2')**2 + fn.col('handAcc16_3')**2),\n",
    "        'chest_acc_magnitude': fn.sqrt(fn.col('chestAcc16_1')**2 + fn.col('chestAcc16_2')**2 + fn.col('chestAcc16_3')**2),\n",
    "        'ankle_acc_magnitude': fn.sqrt(fn.col('ankleAcc16_1')**2 + fn.col('ankleAcc16_2')**2 + fn.col('ankleAcc16_3')**2),\n",
    "\n",
    "        'hand_gyro_magnitude': fn.sqrt(fn.col('handGyro1')**2 + fn.col('handGyro2')**2 + fn.col('handGyro3')**2),\n",
    "        'chest_gyro_magnitude': fn.sqrt(fn.col('chestGyro1')**2 + fn.col('chestGyro2')**2 + fn.col('chestGyro3')**2),\n",
    "        'ankle_gyro_magnitude': fn.sqrt(fn.col('ankleGyro1')**2 + fn.col('ankleGyro2')**2 + fn.col('ankleGyro3')**2)\n",
    "    })\n",
    "\n",
    "    print(\"Đã tạo 6 features magnitude mới (hiệu năng cao)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84613d74-df06-4bd2-b702-e6bcc3925d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    \"\"\"\n",
    "    Cân bằng dataset bằng stratified sampling theo activityID\n",
    "    \"\"\"\n",
    "    print(\"Đang cân bằng dataset...\")\n",
    "\n",
    "    # Đếm số lượng mẫu cho mỗi activity\n",
    "    activity_counts = df.groupBy(\"activityID\").count().cache()\n",
    "    activity_stats = activity_counts.collect()\n",
    "\n",
    "    # Tìm số lượng mẫu nhỏ nhất\n",
    "    min_samples = min(row[\"count\"] for row in activity_stats)\n",
    "\n",
    "    print(\"Phân bố activities:\")\n",
    "    for row in activity_stats:\n",
    "        print(f\"Activity {row['activityID']}: {row['count']} samples\")\n",
    "\n",
    "    # Ánh xạ tỉ lệ sample cần thiết cho từng activity\n",
    "    fractions = {row[\"activityID\"]: min_samples / row[\"count\"] for row in activity_stats}\n",
    "\n",
    "    # Thực hiện stratified sampling\n",
    "    balanced_df = df.stat.sampleBy(\"activityID\", fractions=fractions, seed=42)\n",
    "\n",
    "    print(f\"Dataset sau khi cân bằng: {balanced_df.count()} samples\")\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efe1975-72b3-4c9d-8a63-f3a4dcb925c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_ml(df):\n",
    "    \"\"\"\n",
    "    Chuẩn bị dữ liệu cho machine learning: VectorAssembler + MinMaxScaler + StringIndexer\n",
    "    \"\"\"\n",
    "    print(\"Đang chuẩn bị dữ liệu cho ML...\")\n",
    "\n",
    "    # Xác định feature columns\n",
    "    feature_cols = [col for col in df.columns if col != 'activityID']\n",
    "\n",
    "    # Khởi tạo các transformers\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaled_features\"\n",
    "    )\n",
    "\n",
    "    label_indexer = StringIndexer(\n",
    "        inputCol=\"activityID\",\n",
    "        outputCol=\"label\"\n",
    "    )\n",
    "\n",
    "    # Xây dựng pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, label_indexer])\n",
    "\n",
    "    # Fit và transform dữ liệu\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df_transformed = pipeline_model.transform(df)\n",
    "\n",
    "    # Lọc các cột quan trọng\n",
    "    final_df = df_transformed.select(\"scaled_features\", \"label\", \"activityID\")\n",
    "\n",
    "    print(\"Dữ liệu đã sẵn sàng cho Machine Learning\")\n",
    "    return final_df, pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdfcb6c-c183-404a-b5fc-fc476ca0a49b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m hdfs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020/user/student/pamap2/protocol/*.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020/user/student/pamap2/protocol/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mcreate_spark_session\u001b[0;34m(app_name, master_url, executor_memory, driver_memory, hdfs_url)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_spark_session\u001b[39m(app_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAMAP2 Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                          master_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://spark-master:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                          executor_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2g\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                          driver_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2g\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                          hdfs_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_memory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver_memory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.defaultFS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdfs_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:329\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, jconf):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hdfs_path = \"hdfs://namenode:8020/user/student/pamap2/protocol/*.dat\"\n",
    "path = \"hdfs://namenode:8020/user/student/pamap2/protocol/\"\n",
    "df = load_data(spark, hdfs_path)\n",
    "df = basic_cleaning(df)\n",
    "df = remove_irrelevant_features(df)\n",
    "df = fill_missing_values_ffill_bfill(df)\n",
    "outlier_cols = [c for c in df.columns if c != 'activityID']\n",
    "df = remove_outliers_iqr(df, outlier_cols)\n",
    "df = create_feature_engineering(df)\n",
    "df = balance_dataset(df)\n",
    "final_df, pipeline_model = prepare_for_ml(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62ac1d2-115b-4150-bc5e-f5c0074f2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs://namenode:8020/user/student/pamap2/protocol/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f33bba-6021-4bff-a81c-c1b4802108d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|8.38 0 104 30 2.37223 8.60074 3.51048 2.43954 8.76165 3.35465 -0.0922174 0.0568115 -0.0158445 14.6806 -69.2128 -5.58905 1 0 0 0 31.8125 0.23808 9.80003 -1.68896 0.265304 9.81549 -1.41344 -0.00506495 -0.00678097 -0.00566295 0.47196 -51.0499 43.2903 1 0 0 0 30.3125 9.65918 -1.65569 -0.0997967 9.64689 -1.55576 0.310404 0.00830026 0.00925038 -0.0175803 -61.1888 -38.9599 -58.1438 1 0 0 0|\n",
      "|8.39 0 NaN 30 2.18837 8.5656 3.66179 2.39494 8.55081 3.64207 -0.0244132 0.0477585 0.00647434 14.8991 -69.2224 -5.82311 1 0 0 0 31.8125 0.31953 9.61282 -1.49328 0.234939 9.78539 -1.42846 0.013685 0.00148646 -0.0415218 1.0169 -50.3966 43.1768 1 0 0 0 30.3125 9.6937 -1.57902 -0.215687 9.6167 -1.6163 0.280488 -0.00657665 -0.00463778 0.00036825 -59.8479 -38.8919 -58.5253 1 0 0 0         |\n",
      "|8.4 0 NaN 30 2.37357 8.60107 3.54898 2.30514 8.53644 3.7328 -0.0579761 0.0325743 -0.00698815 14.242 -69.5197 -5.12442 1 0 0 0 31.8125 0.235593 9.72421 -1.76621 0.17385 9.72528 -1.51894 -0.0399232 0.0340559 -0.002113 0.383136 -51.8336 43.7782 1 0 0 0 30.3125 9.58944 -1.73276 0.0929141 9.63173 -1.58605 0.280311 0.00301426 0.000148236 0.022495 -60.7361 -39.4138 -58.3999 1 0 0 0        |\n",
      "|8.41 0 NaN 30 2.07473 8.52853 3.66021 2.33528 8.53622 3.73277 -0.0023516 0.0328098 -0.00374727 14.8908 -69.5439 -6.17367 1 0 0 0 31.8125 0.388697 9.53572 -1.7241 0.157969 9.64994 -1.57952 0.00751315 -0.0104983 -0.020684 0.3154 -49.8144 43.167 1 0 0 0 30.3125 9.58814 -1.7704 0.0545449 9.63197 -1.63135 0.340997 0.00317498 -0.0203009 0.0112754 -60.4091 -38.7635 -58.3956 1 0 0 0        |\n",
      "|8.42 0 NaN 30 2.22936 8.83122 3.7 2.23055 8.59741 3.76295 0.0122691 0.018305 -0.0533248 15.5612 -68.8196 -6.28927 1 0 0 0 31.8125 0.3158 9.49908 -1.60914 0.233506 9.57411 -1.44418 -0.00382244 -0.0112166 -0.0259745 -0.297733 -51.8097 43.6453 1 0 0 0 30.3125 9.69771 -1.65625 -0.0608086 9.64699 -1.64647 0.340965 0.0126977 -0.0143027 -0.00282262 -61.5199 -39.3879 -58.2694 1 0 0 0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.text(\"hdfs://namenode:8020/user/student/pamap2/protocol/*.dat\")\n",
    "df_raw.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
